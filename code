import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.svm import LinearSVC
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from tensorflow.keras.callbacks import EarlyStopping
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = set(stopwords.words('english'))


def count_stopwords(text):
    """Function to count stopwords in a text."""
    words = text.split()  
    stopwords_in_text = [word for word in words if word.lower() in stop_words]  
    return len(stopwords_in_text)  

# Download Data
tweet = pd.read_csv('https://raw.githubusercontent.com/laxmimerit/twitter-disaster-prediction-dataset/master/train.csv')

# Exploratory Data Analysis
plt.rcParams['figure.figsize'] = [8, 4]
plt.rcParams['figure.dpi'] = 80

# Target Class Distribution
sns.countplot('target', data=tweet)
plt.title('Disaster Tweet Predictions')
plt.show()

# No of Characters in Tweets Distribution
tweet['char_counts'] = tweet['text'].apply(len)
sns.distplot(tweet['char_counts'])
plt.show()

# No of Words, Avg Word Length, Stop Words Distribution
tweet['avg_wordlength'] = tweet['text'].apply(lambda x: np.mean([len(word) for word in x.split()]))
stop_words = set(stopwords.words('english'))
tweet['stopwords_counts'] = tweet['text'].apply(lambda x: len([word for word in x.split() if word.lower() in stop_words]))
sns.kdeplot(tweet[tweet['target'] == 1]['char_counts'], shade=True, color='red')
sns.kdeplot(tweet[tweet['target'] == 0]['char_counts'], shade=True, color='magenta')
plt.show()

# Most and Least Common Words Distribution
word_cloud_real = WordCloud(max_font_size=100).generate(' '.join(tweet[tweet['target'] == 1]['text']))
plt.imshow(word_cloud_real)
plt.axis('off')
plt.show()

word_cloud_nonreal = WordCloud(max_font_size=100).generate(' '.join(tweet[tweet['target'] == 0]['text']))
plt.imshow(word_cloud_nonreal)
plt.axis('off')
plt.show()

# Classification with TF-IDF and SVM
text = tweet['text']
y = tweet['target']
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(text)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)
clf = LinearSVC()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print('Classification Report for TF-IDF and SVM')
print(classification_report(y_test, y_pred))

# Classification with Deep Learning and Word Embeddings
token = Tokenizer()
token.fit_on_texts(text)
vocab_size = len(token.word_index) + 1
encoded_text = token.texts_to_sequences(text)
max_length = 40
X = pad_sequences(encoded_text, maxlen=max_length, padding='post')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

# LSTM Model
embedding_dim = 100
model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

# Early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Plot training history
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')
plt.show()

# Prediction
y_pred_probs = model.predict(X_test)
y_pred = (y_pred_probs > 0.5).astype(int)
print('Classification Report for LSTM Model')
print(classification_report(y_test, y_pred))

# Chatbot function
def disaster_tweet_predictor(tweet):
    encoded_tweet = token.texts_to_sequences([tweet])
    padded_tweet = pad_sequences(encoded_tweet, maxlen=max_length, padding='post')
    prediction_prob = model.predict(padded_tweet)[0][0]
    if prediction_prob > 0.5:
        return "This tweet is predicting a disaster."
    else:
        return "This tweet is not predicting a disaster."

# Continuous user input until "exit"
while True:
    user_input = input("Enter a tweet (type 'exit' to quit): ")
    if user_input.lower() == 'exit':
        break
    prediction = disaster_tweet_predictor(user_input)
    print(prediction)
